[
{
	"uri": "//localhost:1313/5-security-monitoring-and-response/1-logging-with-cloudwatch/",
	"title": "Container Logging to CloudWatch with Fluent Bit",
	"tags": [],
	"description": "",
	"content": "Main Content Fluent Bit + CloudWatch Overview Environment Preparation Deploying Fluent Bit to EKS Configuring CloudWatch Output Checking Logs on CloudWatch Troubleshooting Notes \u0026amp; Extended Practice Fluent Bit + CloudWatch Overview Fluent Bit is a lightweight, high-performance log collector and forwarder. On EKS, it helps:\nCollect logs from all containers in the cluster Transform and enrich logs with Kubernetes metadata Send logs to CloudWatch Logs in standard JSON format Support filtering and routing logs by namespace, labels Advantages:\nLightweight: Only ~15MB memory footprint High Performance: Process thousands of events per second Flexible: Support multiple destinations (CloudWatch, Elasticsearch, Kafka, S3\u0026hellip;) Cloud-native: Built-in integration with Kubernetes Environment Preparation Check EKS cluster # Check cluster status kubectl cluster-info # Check nodes kubectl get nodes # Create logging namespace kubectl create namespace logging Create test application (optional) # Create log generator for testing kubectl create deployment log-generator --image=busybox --namespace=default kubectl patch deployment log-generator -p \u0026#39;{\u0026#34;spec\u0026#34;:{\u0026#34;template\u0026#34;:{\u0026#34;spec\u0026#34;:{\u0026#34;containers\u0026#34;:[{\u0026#34;name\u0026#34;:\u0026#34;busybox\u0026#34;,\u0026#34;command\u0026#34;:[\u0026#34;sh\u0026#34;,\u0026#34;-c\u0026#34;,\u0026#34;while true; do echo \\\u0026#34;[$(date)] Test log message #$((i++)) - This is a test log for CloudWatch\\\u0026#34;; sleep 10; done\u0026#34;],\u0026#34;image\u0026#34;:\u0026#34;busybox\u0026#34;}]}}}}\u0026#39; Deploying Fluent Bit to EKS Step 1: Create IAM role for Fluent Bit Create IAM Policy with CloudWatch Logs write permissions:\n{ \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;logs:PutLogEvents\u0026#34;, \u0026#34;logs:DescribeLogStreams\u0026#34;, \u0026#34;logs:DescribeLogGroups\u0026#34;, \u0026#34;logs:CreateLogStream\u0026#34;, \u0026#34;logs:CreateLogGroup\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;*\u0026#34; } ] } Create IAM Role and attach policy:\n# Create trust policy for IRSA cat \u0026gt; trust-policy.json \u0026lt;\u0026lt; EOF { \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Principal\u0026#34;: { \u0026#34;Federated\u0026#34;: \u0026#34;arn:aws:iam::YOUR_ACCOUNT_ID:oidc-provider/oidc.eks.ap-southeast-1.amazonaws.com/id/YOUR_OIDC_ID\u0026#34; }, \u0026#34;Action\u0026#34;: \u0026#34;sts:AssumeRoleWithWebIdentity\u0026#34;, \u0026#34;Condition\u0026#34;: { \u0026#34;StringEquals\u0026#34;: { \u0026#34;oidc.eks.ap-southeast-1.amazonaws.com/id/YOUR_OIDC_ID:sub\u0026#34;: \u0026#34;system:serviceaccount:logging:fluent-bit-sa\u0026#34; } } } ] } EOF # Create IAM Role aws iam create-role \\ --role-name FluentBitRole \\ --assume-role-policy-document file://trust-policy.json # Attach policy aws iam attach-role-policy \\ --role-name FluentBitRole \\ --policy-arn arn:aws:iam::YOUR_ACCOUNT_ID:policy/FluentBitCloudWatchPolicy Step 2: Install Fluent Bit using Helm # Add Fluent Bit Helm repository helm repo add fluent https://fluent.github.io/helm-charts helm repo update # Install Fluent Bit with CloudWatch enabled helm install fluent-bit fluent/fluent-bit \\ --namespace logging \\ --create-namespace \\ --set serviceAccount.create=true \\ --set serviceAccount.name=fluent-bit-sa \\ --set serviceAccount.annotations.\u0026#34;eks\\.amazonaws\\.com/role-arn\u0026#34;=\u0026#34;arn:aws:iam::YOUR_ACCOUNT_ID:role/FluentBitRole\u0026#34; Step 3: Check deployment status # Check pod status kubectl get pods -n logging # View Fluent Bit logs kubectl logs -n logging -l app.kubernetes.io/name=fluent-bit --tail=100 # Check service account kubectl describe serviceaccount fluent-bit-sa -n logging Configuring CloudWatch Output Create ConfigMap for Fluent Bit apiVersion: v1 kind: ConfigMap metadata: name: fluent-bit-config namespace: logging data: fluent-bit.conf: | [SERVICE] Flush 1 Log_Level info Daemon off Parsers_File parsers.conf HTTP_Server On HTTP_Listen 0.0.0.0 HTTP_Port 2020 [INPUT] Name tail Path /var/log/containers/*.log Parser docker Tag kube.* Refresh_Interval 5 Mem_Buf_Limit 50MB Skip_Long_Lines On [FILTER] Name kubernetes Match kube.* Kube_URL https://kubernetes.default.svc:443 Kube_CA_File /var/run/secrets/kubernetes.io/serviceaccount/ca.crt Kube_Token_File /var/run/secrets/kubernetes.io/serviceaccount/token Kube_Tag_Prefix kube.var.log.containers. Merge_Log On Keep_Log Off K8S-Logging.Parser On K8S-Logging.Exclude On [OUTPUT] Name cloudwatch_logs Match * region ap-southeast-1 log_group_name /aws/containerinsights/your-cluster-name/application log_stream_prefix fluent-bit- auto_create_group On log_retention_days 7 parsers.conf: | [PARSER] Name docker Format json Time_Key time Time_Format %Y-%m-%dT%H:%M:%S.%L Time_Keep On Apply configuration kubectl apply -f fluent-bit-config.yaml # Restart Fluent Bit to load new config kubectl rollout restart daemonset/fluent-bit -n logging Checking Logs on CloudWatch 1. Access CloudWatch Console Open CloudWatch Console Select Log groups from the left menu Find log group: /aws/containerinsights/your-cluster-name/application 2. Check log streams # Create some test logs kubectl run test-pod --image=nginx --restart=Never kubectl logs test-pod # Check logs from demo2 namespace kubectl logs -n demo2 -l app=nginx --tail=10 kubectl logs -n demo2 -l app=backend --tail=10 3. View logs on CloudWatch Example log format on CloudWatch:\n{ \u0026#34;time\u0026#34;: \u0026#34;2025-07-16T05:39:39.654Z\u0026#34;, \u0026#34;stream\u0026#34;: \u0026#34;stdout\u0026#34;, \u0026#34;log\u0026#34;: \u0026#34;[Wed Jul 16 05:39:39 UTC 2025] Test log message #1251 - This is a test log for CloudWatch\u0026#34;, \u0026#34;kubernetes\u0026#34;: { \u0026#34;pod_name\u0026#34;: \u0026#34;nginx-5869d7778c-db2lm\u0026#34;, \u0026#34;namespace_name\u0026#34;: \u0026#34;demo2\u0026#34;, \u0026#34;container_name\u0026#34;: \u0026#34;nginx\u0026#34;, \u0026#34;labels\u0026#34;: { \u0026#34;app\u0026#34;: \u0026#34;nginx\u0026#34;, \u0026#34;version\u0026#34;: \u0026#34;v1.0\u0026#34; }, \u0026#34;host\u0026#34;: \u0026#34;ip-192-168-39-33.ap-southeast-1.compute.internal\u0026#34; } } 4. Filtering logs by namespace View only demo2 namespace logs:\n# Add filter in Fluent Bit config [FILTER] Name grep Match kube.* Regex kubernetes.namespace_name demo2 [OUTPUT] Name cloudwatch_logs Match * region ap-southeast-1 log_group_name /aws/containerinsights/demo2-logs log_stream_prefix demo2- auto_create_group On Troubleshooting Common issues and solutions 1. Fluent Bit fails to start:\n# Check error logs kubectl logs -n logging -l app.kubernetes.io/name=fluent-bit # Check permissions kubectl describe pod -n logging -l app.kubernetes.io/name=fluent-bit 2. No logs visible on CloudWatch:\n# Check AWS credentials kubectl exec -n logging deployment/fluent-bit -- env | grep AWS # Check network connectivity kubectl exec -n logging deployment/fluent-bit -- nslookup logs.ap-southeast-1.amazonaws.com # View detailed Fluent Bit logs kubectl logs -n logging -l app.kubernetes.io/name=fluent-bit | grep -i \u0026#34;cloudwatch\\|error\u0026#34; 3. Incorrect log format:\n# Check parser configuration kubectl get configmap fluent-bit-config -n logging -o yaml # Test parser locally kubectl exec -n logging deployment/fluent-bit -- fluent-bit --dry-run --config /fluent-bit/etc/fluent-bit.conf 4. Permission errors:\n# Check IAM role aws sts get-caller-identity # Check service account annotation kubectl describe serviceaccount fluent-bit-sa -n logging | grep Annotations Useful debug commands # View realtime logs kubectl logs -n logging -l app.kubernetes.io/name=fluent-bit -f # Check metrics kubectl port-forward -n logging svc/fluent-bit 2020:2020 curl localhost:2020/api/v1/metrics # View current configuration kubectl exec -n logging deployment/fluent-bit -- cat /fluent-bit/etc/fluent-bit.conf Notes \u0026amp; Extended Practice Performance optimization # Increase buffer size for high-volume logs [INPUT] Name tail Path /var/log/containers/*.log Mem_Buf_Limit 100MB Buffer_Max_Size 1MB Buffer_Chunk_Size 64KB Log routing by environment # Route logs by namespace [OUTPUT] Name cloudwatch_logs Match kube.var.log.containers.*production* log_group_name /aws/containerinsights/production-logs [OUTPUT] Name cloudwatch_logs Match kube.var.log.containers.*staging* log_group_name /aws/containerinsights/staging-logs Structured logging # Parse JSON logs [FILTER] Name parser Match kube.* Parser json Key_Name log Cost optimization # Reduce retention to save costs [OUTPUT] Name cloudwatch_logs Match * log_retention_days 3 # Exclude debug logs [FILTER] Name grep Match * Exclude log (DEBUG|TRACE) Monitoring Fluent Bit # Enable metrics endpoint [SERVICE] HTTP_Server On HTTP_Listen 0.0.0.0 HTTP_Port 2020 # Prometheus metrics [OUTPUT] Name prometheus_exporter Match * Host 0.0.0.0 Port 2021 Alternative: OpenTelemetry Collector For larger systems, consider:\n# Install OpenTelemetry Operator kubectl apply -f https://github.com/open-telemetry/opentelemetry-operator/releases/latest/download/opentelemetry-operator.yaml # Configure OpenTelemetry Collector apiVersion: opentelemetry.io/v1alpha1 kind: OpenTelemetryCollector metadata: name: otel-collector spec: config: | receivers: filelog: include: [\u0026#34;/var/log/containers/*.log\u0026#34;] exporters: awscloudwatchlogs: region: ap-southeast-1 log_group_name: \u0026#34;/aws/containerinsights/otel-logs\u0026#34; service: pipelines: logs: receivers: [filelog] exporters: [awscloudwatchlogs] Important Notes:\nMonitor CloudWatch Logs costs (charged per GB ingested) Consider appropriate log retention policy Use log aggregation and sampling for production workloads Combine with CloudWatch Insights to query and analyze logs 👉 Next: 5.2 Security Dashboard → to set up monitoring and alerting for cluster security.\n"
},
{
	"uri": "//localhost:1313/4-policy-enforcement-and-runtime-protection/1-opa-gatekeeper/",
	"title": "Policy Management with OPA Gatekeeper (Windows)",
	"tags": [],
	"description": "",
	"content": "Main Content Introduction to OPA Gatekeeper Installing Helm on Windows Deploying Gatekeeper on EKS Writing and testing policies Introduction OPA (Open Policy Agent) is a tool used to enforce policies across multiple systems. Gatekeeper is an extension project that helps apply OPA to Kubernetes.\nGatekeeper helps with:\nControlling deployments based on rules (constraints) Logging policy violations Preventing unsafe configurations (e.g., banning images with :latest tag) Key Benefits Admission Control: Validates resources before they\u0026rsquo;re created Audit Mode: Identifies existing violations without blocking Custom Policies: Write policies using Rego language Mutation: Automatically modify resources to comply with policies Installing Helm on Windows Method 1: Manual Installation Visit https://github.com/helm/helm/releases Download file: helm-v3.x.x-windows-amd64.zip Extract → Get the helm.exe file Create directory e.g.: C:\\Program Files\\helm\\ → copy helm.exe into it Add the path to Environment Variables \u0026gt; Path Open PowerShell/CMD and verify: helm version Method 2: Using Package Managers Using Chocolatey:\nchoco install kubernetes-helm Using Scoop:\nscoop install helm Using winget:\nwinget install Helm.Helm ✅ If version appears, installation is successful.\nDeploying Gatekeeper on EKS Step 1: Add Gatekeeper Helm repository helm repo add gatekeeper https://open-policy-agent.github.io/gatekeeper/charts helm repo update Step 2: Install Gatekeeper helm install gatekeeper gatekeeper/gatekeeper \\ --namespace gatekeeper-system \\ --create-namespace Step 3: Verify pod status kubectl get pods -n gatekeeper-system Expected output should show pods in Running state:\nNAME READY STATUS RESTARTS AGE\rgatekeeper-audit-7f9c8b8b8b-xxxxx 1/1 Running 0 2m\rgatekeeper-controller-manager-xxxxx-xxxxx 1/1 Running 0 2m\rgatekeeper-controller-manager-xxxxx-yyyyy 1/1 Running 0 2m 📸 Take a screenshot of the Running status.\nStep 4: Verify Gatekeeper installation kubectl get crd | grep gatekeeper kubectl api-resources | grep gatekeeper Writing and testing policies Step 1: Create template.yaml file apiVersion: templates.gatekeeper.sh/v1beta1 kind: ConstraintTemplate metadata: name: k8srequiredimagetag spec: crd: spec: names: kind: K8sRequiredImageTag validation: properties: allowedTags: type: array items: type: string targets: - target: admission.k8s.gatekeeper.sh rego: | package k8srequiredimagetag violation[{\u0026#34;msg\u0026#34;: msg}] { container := input.review.object.spec.containers[_] endswith(container.image, \u0026#34;:latest\u0026#34;) msg := \u0026#34;Container image must not use \u0026#39;latest\u0026#39; tag\u0026#34; } violation[{\u0026#34;msg\u0026#34;: msg}] { container := input.review.object.spec.containers[_] not contains(container.image, \u0026#34;:\u0026#34;) msg := \u0026#34;Container image must specify a tag\u0026#34; } Apply the template:\nkubectl apply -f template.yaml Step 2: Create constraint.yaml file apiVersion: constraints.gatekeeper.sh/v1beta1 kind: K8sRequiredImageTag metadata: name: disallow-latest-tag spec: match: kinds: - apiGroups: [\u0026#34;\u0026#34;] kinds: [\u0026#34;Pod\u0026#34;] - apiGroups: [\u0026#34;apps\u0026#34;] kinds: [\u0026#34;Deployment\u0026#34;] parameters: allowedTags: [\u0026#34;v1.0\u0026#34;, \u0026#34;stable\u0026#34;, \u0026#34;production\u0026#34;] Apply the constraint:\nkubectl apply -f constraint.yaml Step 3: Test with a violating pod Create test-pod.yaml:\napiVersion: v1 kind: Pod metadata: name: test-pod spec: containers: - name: nginx image: nginx:latest Try to apply:\nkubectl apply -f test-pod.yaml ➡️ Expected Result: Pod will be rejected due to policy violation.\nError message should be similar to:\nerror validating data: ValidationError(Pod.spec.containers[0].image): Container image must not use \u0026#39;latest\u0026#39; tag Step 4: Test with a compliant pod Create compliant-pod.yaml:\napiVersion: v1 kind: Pod metadata: name: compliant-pod spec: containers: - name: nginx image: nginx:1.21 Apply:\nkubectl apply -f compliant-pod.yaml ✅ Expected Result: Pod should be created successfully.\nAdvanced Policy Examples 1. Require resource limits apiVersion: templates.gatekeeper.sh/v1beta1 kind: ConstraintTemplate metadata: name: k8srequireresources spec: crd: spec: names: kind: K8sRequireResources targets: - target: admission.k8s.gatekeeper.sh rego: | package k8srequireresources violation[{\u0026#34;msg\u0026#34;: msg}] { container := input.review.object.spec.containers[_] not container.resources.limits.memory msg := \u0026#34;Container must specify memory limits\u0026#34; } violation[{\u0026#34;msg\u0026#34;: msg}] { container := input.review.object.spec.containers[_] not container.resources.limits.cpu msg := \u0026#34;Container must specify CPU limits\u0026#34; } 2. Enforce security contexts apiVersion: templates.gatekeeper.sh/v1beta1 kind: ConstraintTemplate metadata: name: k8srequiresecuritycontext spec: crd: spec: names: kind: K8sRequireSecurityContext targets: - target: admission.k8s.gatekeeper.sh rego: | package k8srequiresecuritycontext violation[{\u0026#34;msg\u0026#34;: msg}] { container := input.review.object.spec.containers[_] not container.securityContext.runAsNonRoot msg := \u0026#34;Container must run as non-root user\u0026#34; } violation[{\u0026#34;msg\u0026#34;: msg}] { container := input.review.object.spec.containers[_] container.securityContext.privileged msg := \u0026#34;Container must not run in privileged mode\u0026#34; } Monitoring and Troubleshooting Check constraint status kubectl get constraints kubectl describe k8srequiredimagetag disallow-latest-tag View audit violations kubectl get constraints -o yaml | grep -A 10 violations Debug policy issues # Check Gatekeeper logs kubectl logs -n gatekeeper-system deployment/gatekeeper-controller-manager # Check if templates are valid kubectl get constrainttemplate k8srequiredimagetag -o yaml Best Practices Policy Development Start with Audit Mode: Test policies without enforcement first Gradual Rollout: Apply policies to non-critical namespaces first Clear Error Messages: Write descriptive violation messages Test Thoroughly: Validate policies with various resource configurations Performance Considerations Efficient Rego: Write optimized Rego policies Resource Limits: Set appropriate limits for Gatekeeper components Monitoring: Monitor Gatekeeper performance and admission latency Security Namespace Exclusions: Exclude system namespaces when appropriate RBAC: Implement proper RBAC for policy management Policy Reviews: Regular review and update of policies ✅ Notes:\nGatekeeper is highly effective in controlling deployments Policies can be extended and customized based on requirements Integration with CI/CD pipelines helps catch violations early Regular monitoring and updates ensure policy effectiveness Next: 4.2 Runtime Monitoring with Falco →\n"
},
{
	"uri": "//localhost:1313/3-container-security-hardening/1-image-scanning-with-trivy/",
	"title": "Container Image Vulnerability Scanning with Trivy (via Docker)",
	"tags": [],
	"description": "",
	"content": "Main Content Introduction to Trivy Installing and Using Trivy with Docker on Windows Scanning Docker Images and Analyzing Results Notes and Best Practices Introduction to Trivy Trivy is an open-source tool that supports scanning for:\nSecurity vulnerabilities in container images (CVE) Configuration errors in IaC such as Kubernetes YAML, Terraform, etc. Standard output formats: table, JSON, SARIF\u0026hellip; Installing and Using Trivy with Docker on Windows ✅ Prerequisites Docker Desktop for Windows installed (WSL2 or Hyper-V) Ensure Docker is running 🐳 Running Trivy Directly from Docker Sample command (scanning nginx image):\ndocker run --rm -v /var/run/docker.sock:/var/run/docker.sock ` -v $env:USERPROFILE\\.trivy-cache:/root/.cache/ ` aquasec/trivy:latest image nginx 💡 Explanation:\n\u0026ndash;rm: removes container after execution -v /var/run/docker.sock:/var/run/docker.sock: allows Trivy to access images on the host machine -v $env:USERPROFILE.trivy-cache:/root/.cache/: mounts cache directory to save time on subsequent scans aquasec/trivy:latest: uses the official image from Docker Hub image nginx: specifies the command and image name to scan 🖼 Real-world example with a vulnerable image:\ndocker run --rm -v /var/run/docker.sock:/var/run/docker.sock ` -v $env:USERPROFILE\\.trivy-cache:/root/.cache/ ` aquasec/trivy:latest image nginxinc/nginx-unprivileged:1.24.0-alpine-slim Scanning Docker Images and Analyzing Results After running the command, you will receive results in table format: Total: 27 (UNKNOWN: 2, LOW: 2, MEDIUM: 20, HIGH: 3)\nLibrary Vulnerability Severity Installed Fixed Title libcrypto3 CVE-2024-6119 HIGH 3.1.4-r6 3.1.7-r0 openssl: DoS in X.509 name checks nginx CVE-2023-44487 HIGH 1.24.0-r1 1.24.0-r7 HTTP/2 vulnerability leading to DDoS attacks busybox CVE-2023-42366 MEDIUM 1.36.1-r5 1.36.1-r6 Heap-buffer-overflow in busybox awk ⚠️ Despite being a slim and popular image, it still contains many security vulnerabilities (HIGH, MEDIUM). This demonstrates that regular scanning is essential.\nWhen running the scan command, you will receive a detailed table of affected packages, severity levels, and patched versions.\nExample analysis:\nCVE-2024-6119 (HIGH): affects openssl causing denial of service (DoS) CVE-2023-44487 (HIGH): affects HTTP/2, potentially vulnerable to DDoS CVE-2023-42366 (MEDIUM): heap buffer overflow error ✅ How to handle:\nAlways prioritize fixing CRITICAL and HIGH vulnerabilities If possible, use newer versions of the image (1.24.0-r7, 1.25.x, \u0026hellip;) Avoid using :latest, pin specific versions for security control Notes and Best Practices Exporting result files:\ndocker run --rm ` -v /var/run/docker.sock:/var/run/docker.sock ` -v \u0026#34;$env:USERPROFILE/.trivy-cache:/root/.cache\u0026#34; ` -v \u0026#34;$PWD:/root/report\u0026#34; ` aquasec/trivy:latest image -f json -o /root/report/result.json nginx ➡️ JSON results will be saved at D:\\pj_aws\\result.json\nCI/CD Integration: You can integrate the above scan command into:\nGitHub Actions GitLab CI/CD Jenkins, CircleCI\u0026hellip; ✅ This helps automatically check security with each build or deployment.\nAdditional Best Practices: Regular Scanning Schedule\nSet up automated daily/weekly scans Monitor new CVE databases for updates Image Selection Strategy\nUse official images from trusted registries Prefer minimal base images (alpine, distroless) Keep base images updated regularly Vulnerability Management\nCreate vulnerability remediation workflows Set security thresholds (e.g., no HIGH/CRITICAL in production) Document accepted risks for non-fixable vulnerabilities Integration with Security Tools\nCombine with admission controllers in Kubernetes Use with registry scanning solutions Integrate with security incident response processes 🎯 Conclusion: Trivy is an indispensable tool when deploying containers in production. You should scan images regularly, use official images, keep them updated, and integrate CI/CD to ensure system security.\n"
},
{
	"uri": "//localhost:1313/2-setup-aws-eks/1-create-eks-cluster/",
	"title": "Creating EKS Cluster on AWS",
	"tags": [],
	"description": "",
	"content": "Main Content:\nStep 1: Install tools on Windows Step 2: Create cluster configuration file Step 3: Create cluster using command Step 4: Verify connection with kubectl Notes and Important Points Step 1: Install tools on Windows AWS CLI Visit: https://docs.aws.amazon.com/cli/latest/userguide/install-cliv2-windows.html → Download the .msi file and install it like any regular software. Then open CMD and verify:\naws --version If the version appears, the installation was successful.\n⚙️ Configure AWS account:\nCreate IAM USER Create Access keys/Secret Access Key Open PowerShell aws configure Then enter the following information: AWS Access Key ID AWS Secret Access Key Region: ap-southeast-1 Output format: json Install kubectl Visit the official link: 👉 https://kubernetes.io/docs/tasks/tools/install-kubectl-windows/ Or use choco if you have Chocolatey installed:\nchoco install kubernetes-cli Verify the installation:\nkubectl version --client Install eksctl Visit: 👉 https://eksctl.io/introduction/#installation Download the .exe file → Rename it to eksctl.exe and:\nCopy to a directory like C:\\Program Files\\eksctl Add that directory to Environment Variables \u0026gt; PATH Or use choco if you have Chocolatey installed:\nchoco install eksctl Verify the installation: eksctl version Step 2: Create cluster configuration file Open Notepad or VS Code, create a file named eks-cluster.yaml with the following content:\napiVersion: eksctl.io/v1alpha5 kind: ClusterConfig metadata: name: secure-eks region: ap-southeast-1 nodeGroups: - name: ng-1 instanceType: t3.medium desiredCapacity: 2 ssh: allow: true Save it in the same directory where you\u0026rsquo;ll open the terminal later.\nStep 3: Create cluster using command Open PowerShell or terminal in VS Code, navigate to the directory containing the eks-cluster.yaml file, then run:\neksctl create cluster -f eks-cluster.yaml You might encounter this error: Error: cannot find EC2 key pair \u0026ldquo;~/.ssh/id_rsa.pub\u0026rdquo;\n💡 Cause: In the eks-cluster.yaml configuration file, you have enabled SSH for the node group:\nssh: allow: true When allow: true, eksctl will try to find the ~/.ssh/id_rsa.pub file to use as the SSH key pair – but you haven\u0026rsquo;t created this key on your Windows machine yet.\nYou need to manually create the SSH key pair. Open PowerShell and run:\nssh-keygen Press Enter repeatedly to accept the default path (C:\\Users\u0026lt;username\u0026gt;.ssh\\id_rsa)\nThen verify the file:\ntype $env:USERPROFILE\\.ssh\\id_rsa.pub Run the cluster creation command again:\neksctl create cluster -f eks-cluster.yaml 📌 eksctl will automatically use the id_rsa.pub file and create the corresponding key pair in AWS EC2.\n⏳ The creation process may take 10–15 minutes. eksctl will create: VPC, IAM role, Security Group, Control Plane, EC2 Nodes\u0026hellip;\nStep 4: Verify connection with kubectl After the cluster is created, verify the connection:\nkubectl get nodes ✅ You should see 2 worker nodes running – EKS is ready to use!\n📸 Take a screenshot of the terminal results to use in your workshop report.\nNotes and Important Points eksctl will automatically create VPC and node groups if they don\u0026rsquo;t exist It\u0026rsquo;s recommended to use region ap-southeast-1 (Singapore) if you\u0026rsquo;re in Vietnam to reduce latency The cluster creation process is fully automated through eksctl Make sure your AWS credentials have sufficient permissions to create EKS resources Keep your SSH keys secure as they provide access to your worker nodes "
},
{
	"uri": "//localhost:1313/1-intro-to-container-security/",
	"title": "Introduction to Container Security",
	"tags": [],
	"description": "",
	"content": "🎯 Chapter Objectives In this first chapter, you will be introduced to the most important concepts of container security, covering both theoretical overviews and hands-on threat analysis skills. This serves as the foundation for the entire workshop.\n📚 Main Topics 1.1 Overview of Container Security\nLearn what containers are, the different layers of container security throughout its lifecycle, and get introduced to tools used in the workshop such as Falco, OPA, NetworkPolicy, Trivy, etc.\n1.2 Threat Modeling\nExplore the STRIDE threat modeling framework, identify attack surfaces in a containerized system, and practice creating a threat model diagram for a sample containerized application.\n✅ Learning Outcomes By the end of this chapter, you will:\nUnderstand why containers must be secured properly Grasp the overall container security model from build to runtime Be able to identify potential threats using the STRIDE model Be ready to start hands-on container security deployment on AWS 🚀 Let’s begin with section 1.1!\n"
},
{
	"uri": "//localhost:1313/1-intro-to-container-security/1-overview/",
	"title": "Overview of Container Security",
	"tags": [],
	"description": "",
	"content": "Main Contents:\nWhat is a container and why secure it? Container security layers Final goals of the workshop What is a container and why secure it? A container is a method of packaging an application along with its libraries and configurations so it can run anywhere. However, due to its lightweight nature and shared kernel, containers pose several security risks:\nContainers may access host file systems if not properly restricted “Container escape” attacks can compromise the entire node Misconfigurations (e.g., exposed ports, privileged mode) are easily exploited Images from unverified sources may contain malware Container security layers Container security must cover the entire application lifecycle, not just the runtime:\nStage Security Measure Build Scan images (e.g., Trivy), remove vulnerabilities Deploy Apply policy controls (e.g., OPA, limits) Runtime Detect anomalies (e.g., Falco) Network Restrict access (e.g., NetworkPolicy) Monitoring Alerting \u0026amp; incident response Final goals of the workshop By the end of the workshop, you will be able to:\nSet up an EKS cluster with a containerized application Perform vulnerability scanning and benchmark checks (CIS) Apply NetworkPolicy, OPA, and Falco for container protection Deploy alerting systems and automate incident remediation Understand and apply threat modeling (STRIDE, ATT\u0026amp;CK) 🧠 You will take screenshots of each step and include them in the workshop content to guide others in building a comprehensive container security environment.\n"
},
{
	"uri": "//localhost:1313/4-policy-enforcement-and-runtime-protection/2-falco-setup-and-rule/",
	"title": "Runtime Monitoring with Falco (Windows)",
	"tags": [],
	"description": "",
	"content": "Main Content Introduction to Falco Installing Falco with Helm Testing runtime alerts Introduction to Falco Falco is a real-time behavioral monitoring tool for Kubernetes. It helps detect:\nUnusual system access (e.g., /etc/passwd) Shell usage in containers (bash, sh) Modification of sensitive files Network anomalies and suspicious connections Privilege escalation attempts Falco uses kernel syscalls to record behavior and sends alerts based on predefined rules.\nKey Features Real-time Detection: Monitors syscalls at the kernel level Cloud Native: Native integration with Kubernetes Flexible Rules: Customizable rule engine using YAML Multiple Outputs: Supports various alert destinations CNCF Project: Graduated Cloud Native Computing Foundation project How Falco Works Kernel Module/eBPF: Captures system calls from the kernel Rule Engine: Evaluates syscalls against security rules Alert Generation: Generates alerts for suspicious activities Output Channels: Sends alerts to various destinations (logs, webhooks, etc.) Installing Falco with Helm 🔧 Prerequisites: kubectl, eksctl, AWS CLI installed and EKS cluster running Helm installed (see previous section for installation) Step 1: Verify Helm installation helm version Expected output:\nversion.BuildInfo{Version:\u0026#34;v3.x.x\u0026#34;, GitCommit:\u0026#34;...\u0026#34;, GitTreeState:\u0026#34;clean\u0026#34;, GoVersion:\u0026#34;go1.x.x\u0026#34;} Step 2: Add Falco repository and install helm repo add falcosecurity https://falcosecurity.github.io/charts helm repo update Step 3: Create namespace and install Falco kubectl create ns falco helm install falco falcosecurity/falco -n falco Step 4: Verify installation kubectl get pods -n falco Expected output:\nNAME READY STATUS RESTARTS AGE\rfalco-xxxxx 1/1 Running 0 2m 📸 Take a screenshot of Falco pod running in the falco namespace for documentation.\nStep 5: Check Falco configuration kubectl get configmap falco -n falco -o yaml This shows the default rules and configuration.\nTesting runtime alerts Step 1: Create suspicious shell activity kubectl run -i --tty attacker --image=alpine --rm --restart=Never -- sh Once inside the container, run:\n# Try to access sensitive files touch /etc/passwd cat /etc/shadow ls /root # Try to use networking tools nc -l 8080 Step 2: View Falco logs kubectl logs -l app.kubernetes.io/name=falco -n falco --follow ✅ Expected results:\n{\r\u0026#34;output\u0026#34;: \u0026#34;File below /etc opened for writing (user=root command=touch /etc/passwd file=/etc/passwd parent=sh gparent=\u0026lt;NA\u0026gt;)\u0026#34;,\r\u0026#34;priority\u0026#34;: \u0026#34;Error\u0026#34;,\r\u0026#34;rule\u0026#34;: \u0026#34;Write below etc\u0026#34;,\r\u0026#34;time\u0026#34;: \u0026#34;2025-07-15T10:30:45.123456789Z\u0026#34;,\r\u0026#34;output_fields\u0026#34;: {\r\u0026#34;evt.time\u0026#34;: 1642248645123456789,\r\u0026#34;user.name\u0026#34;: \u0026#34;root\u0026#34;,\r\u0026#34;proc.cmdline\u0026#34;: \u0026#34;touch /etc/passwd\u0026#34;,\r\u0026#34;fd.name\u0026#34;: \u0026#34;/etc/passwd\u0026#34;\r}\r} 📸 Take a screenshot of Falco logs containing the alert line for the report.\nStep 3: Test additional scenarios Network activity monitoring:\nkubectl run network-test --image=nicolaka/netshoot --rm -it --restart=Never -- bash Inside the container:\n# These should trigger network-related alerts nmap 10.0.0.1 nc 8.8.8.8 80 File modification monitoring:\nkubectl run file-test --image=ubuntu --rm -it --restart=Never -- bash Inside the container:\n# These should trigger file-related alerts echo \u0026#34;test\u0026#34; \u0026gt; /bin/ls chmod +x /tmp/suspicious-script Step 4: Monitor different alert types # Monitor all Falco alerts with timestamps kubectl logs -l app.kubernetes.io/name=falco -n falco --follow --timestamps Common alert patterns you might see:\nShell spawned in container File below /etc opened for writing Network connection attempted Sensitive file access Understanding Falco Rules Default Rule Categories System Activity\nFile access patterns Process execution Network connections Container Security\nShell spawning Privilege escalation Suspicious binaries Kubernetes Security\nPod operations Service account usage API server interactions Custom Rule Example Create a custom rule file custom-rules.yaml:\ncustomRules: my-rules.yaml: |- - rule: Detect cryptocurrency mining desc: Detect potential cryptocurrency mining activity condition: \u0026gt; spawned_process and (proc.name in (xmrig, cpuminer, ccminer)) output: \u0026gt; Cryptocurrency mining detected (user=%user.name command=%proc.cmdline container=%container.name) priority: CRITICAL Apply the custom rules:\nhelm upgrade falco falcosecurity/falco -n falco -f custom-rules.yaml Advanced Configuration Enable additional outputs Update Falco to send alerts to multiple destinations:\n# values.yaml falco: grpc: enabled: true grpcOutput: enabled: true httpOutput: enabled: true url: \u0026#34;http://webhook-server:8080/falco-alerts\u0026#34; jsonOutput: true jsonIncludeOutputProperty: true helm upgrade falco falcosecurity/falco -n falco -f values.yaml Performance tuning # values.yaml for better performance falco: syscallEventDrops: maxBurst: 1000 rate: 0.03333 priority: INFO # Reduce verbosity bufferSize: 8 # Increase buffer size Monitoring and Alerting Integration Integration with Prometheus # Add Falco exporter for Prometheus metrics helm repo add falcosecurity https://falcosecurity.github.io/charts helm install falco-exporter falcosecurity/falco-exporter -n falco Integration with Grafana Import Falco dashboard ID: 11914 from Grafana.com for visualization.\nSlack Integration # values.yaml for Slack notifications falco: httpOutput: enabled: true url: \u0026#34;https://hooks.slack.com/services/YOUR/SLACK/WEBHOOK\u0026#34; Troubleshooting Common Issues 1. Falco not starting:\nkubectl describe pod -l app.kubernetes.io/name=falco -n falco kubectl logs -l app.kubernetes.io/name=falco -n falco 2. No alerts appearing:\n# Check if Falco is processing events kubectl logs -l app.kubernetes.io/name=falco -n falco | grep \u0026#34;Events processed\u0026#34; # Verify syscall source kubectl logs -l app.kubernetes.io/name=falco -n falco | grep -i \u0026#34;syscall\u0026#34; 3. High CPU usage:\n# Check event rate kubectl top pod -n falco # Reduce rule verbosity or increase buffer size Debug Mode Enable debug logging:\nhelm upgrade falco falcosecurity/falco -n falco --set falco.logLevel=debug Best Practices Rule Management Start with default rules and gradually customize Test rules in non-production environments first Monitor false positives and tune accordingly Regular rule updates to address new threats Performance Optimization Set appropriate log levels to balance security and performance Use rule conditions efficiently to reduce processing overhead Monitor resource usage and scale accordingly Consider event filtering for high-traffic environments Security Considerations Secure Falco configuration with proper RBAC Protect alert channels from tampering Regular security updates for Falco components Audit rule changes and maintain version control Notes Falco detects and alerts but does not prevent malicious behavior For automated response to violations (4.3), you need to combine with Falcosidekick or other response tools Regular monitoring of Falco alerts is essential for effective security posture Integration with SIEM systems enhances threat detection capabilities 👉 Next: 4.3 Automated Response to Violations with Falcosidekick →\n"
},
{
	"uri": "//localhost:1313/3-container-security-hardening/2-cis-benchmark-with-kubebench/",
	"title": "CIS Benchmark Compliance Check with kube-bench",
	"tags": [],
	"description": "",
	"content": "Main Content Introduction to CIS Benchmark and kube-bench Deploying kube-bench on EKS Analyzing Check Results and Recommendations Introduction CIS Kubernetes Benchmark is a standardized security ruleset published by the Center for Internet Security. It includes checks for API Server, Scheduler, Etcd, Kubelet, and more.\nkube-bench is a tool by Aqua Security that automates the process of checking these security controls on Kubernetes clusters.\nDeploying kube-bench on EKS Step 1: Create job YAML file Create a file named kube-bench-job.yaml with the following content:\napiVersion: batch/v1 kind: Job metadata: name: kube-bench spec: template: spec: containers: - name: kube-bench image: aquasec/kube-bench:latest command: [\u0026#34;kube-bench\u0026#34;, \u0026#34;--benchmark\u0026#34;, \u0026#34;eks\u0026#34;] volumeMounts: - name: var-lib-kubelet mountPath: /var/lib/kubelet readOnly: true - name: etc-systemd mountPath: /etc/systemd readOnly: true - name: var-lib-etcd mountPath: /var/lib/etcd readOnly: true restartPolicy: Never volumes: - name: var-lib-kubelet hostPath: path: /var/lib/kubelet - name: etc-systemd hostPath: path: /etc/systemd - name: var-lib-etcd hostPath: path: /var/lib/etcd backoffLimit: 0 Step 2: Apply the Job to the cluster kubectl apply -f kube-bench-job.yaml ⏳ Wait a few minutes for the Job to complete.\nStep 3: View the check results kubectl logs job/kube-bench Analyzing Results The output will display check items according to CIS standards, for example:\n== Summary total ==\r4 checks PASS\r4 checks FAIL\r43 checks WARN Understanding Result Categories ❌ [FAIL] - Critical security issues that must be addressed:\n--anonymous-auth=false: disable anonymous access --authorization-mode=Webhook: configure secure authorization --make-iptables-util-chains=true: properly handle iptables chains 🔧 Remediation: Modify the kubelet.service file on each node according to the guidelines, then run:\nsystemctl daemon-reload systemctl restart kubelet ⚠️ [WARN] - Security concerns that should be addressed:\nExcessive use of wildcards in RBAC (roles, clusterroles) Missing NetworkPolicy for namespaces Pod Security Policy not properly configured → allows privileged containers IAM / ECR / Secrets not properly configured on AWS ✅ [PASS] - Security controls that are properly configured:\nSome kubelet configuration files have correct ownership and permissions (644, root:root) Additional Notes Advanced Usage Options kube-bench can run in host, daemonset, or job mode depending on your needs Target specific nodes using nodeSelector labels Export results to JSON format: kubectl logs job/kube-bench -o json \u0026gt; cis-result.json Integration with CI/CD You can integrate kube-bench into your automation pipeline:\n# Example GitHub Actions step - name: Run CIS Benchmark Check run: | kubectl apply -f kube-bench-job.yaml kubectl wait --for=condition=complete job/kube-bench --timeout=300s kubectl logs job/kube-bench \u0026gt; cis-results.txt Scheduling Regular Checks Create a CronJob for periodic compliance checks:\napiVersion: batch/v1 kind: CronJob metadata: name: kube-bench-cronjob spec: schedule: \u0026#34;0 2 * * 0\u0026#34; # Weekly on Sunday at 2 AM jobTemplate: spec: template: spec: # Same spec as above Job Best Practices ✅ General Best Practices:\nAlways perform regular checks according to CIS Benchmark Avoid using default Kubernetes/Kubelet configurations Prioritize fixing issues in order: FAIL → WARN → maintain PASS status Automate checks by integrating kube-bench into CI/CD or cron jobs Remediation Priority Framework Immediate Action (FAIL)\nCritical security misconfigurations Authentication and authorization issues Network security gaps Planned Remediation (WARN)\nRBAC overprivileging Missing security policies Audit logging configuration Continuous Monitoring (PASS)\nMaintain current secure configurations Regular validation of security controls EKS-Specific Considerations Some CIS controls are managed by AWS and may not be directly configurable Focus on node-level and workload-level security controls Leverage AWS security services (IAM, VPC, Security Groups) for comprehensive security 🎯 Conclusion: Regular CIS Benchmark compliance checking with kube-bench is essential for maintaining a secure Kubernetes environment. Implement automated checking, prioritize remediation based on risk, and maintain continuous security posture improvement.\n"
},
{
	"uri": "//localhost:1313/2-setup-aws-eks/2-deploy-demo-workloads/",
	"title": "Deploy Sample Workload on EKS",
	"tags": [],
	"description": "",
	"content": "Table of Contents Create a separate namespace for the demo Deploy nginx and backend applications Verify connectivity and prepare for upcoming labs Create a separate namespace for the demo Namespaces help isolate resources between different applications. First, create a namespace named demo:\nkubectl create namespace demo 📸 Output:\nThen verify the namespace:\nkubectl get ns 📸 Output:\nDeploy nginx and backend applications The sample workload consists of two Pods:\nnginx (used as frontend)\nhttp-echo (used as backend)\nCreate a file named demo-app.yaml with the following content:\napiVersion: v1 kind: Pod metadata: name: nginx namespace: demo spec: containers: - name: nginx image: nginx ports: - containerPort: 80 --- apiVersion: v1 kind: Pod metadata: name: backend namespace: demo spec: containers: - name: backend image: hashicorp/http-echo args: - \u0026#34;-text=Hello from backend\u0026#34; ports: - containerPort: 5678 ▶️ Apply the configuration:\nkubectl apply -f demo-app.yaml 📸 Output after applying:\nVerify connectivity and prepare for upcoming labs Run the following command to check the status of the Pods:\nkubectl get pods -n demo 📸 Output:\n✅ You should see both nginx and backend Pods in Running state.\nThis sample workload will be used throughout the upcoming labs such as: NetworkPolicy, Falco, OPA, etc.\nOptional: Expose nginx externally To access the nginx pod from outside (e.g., via browser or curl), you can expose it using a LoadBalancer service:\nkubectl expose pod nginx --port=80 --type=LoadBalancer -n demo Notes:\nThis command will create a Service with a public IP (if running on a cloud platform like EKS). You can retrieve the access URL using: kubectl get svc -n demo. 🎉 You have successfully deployed a sample workload – ready for the next security-focused labs!\n"
},
{
	"uri": "//localhost:1313/1-intro-to-container-security/2-threat-modeling/",
	"title": "Threat Modeling",
	"tags": [],
	"description": "",
	"content": "Main Contents:\nWhat is the STRIDE model? Identifying container attack surfaces Drawing a threat model diagram Hands-on threat modeling for a containerized app What is the STRIDE model? STRIDE is a threat modeling framework that helps identify potential threats in a system. It consists of:\nComponent Threat Type S Spoofing – Identity impersonation T Tampering – Data manipulation R Repudiation – Denial of actions I Information Disclosure – Data leakage D Denial of Service – Service disruption E Elevation of Privilege – Privilege escalation 🧠 STRIDE helps analyze each system component to uncover potential vulnerabilities.\nIdentifying container attack surfaces When deploying containerized applications, you must identify exploitable points, such as:\nContainer Image: may contain CVEs or malware Registry: unprotected → vulnerable to unauthorized push/pull Kubernetes API: can be attacked via misconfigured RBAC Pods running as root: increase escalation risks Sensitive information: hardcoded secrets or ENV vars Drawing a threat model diagram You can use the following tools for visualization:\n✏️ Draw.io 🔐 Microsoft Threat Modeling Tool 👉 The diagram should include:\nUsers (e.g., developers, attackers) Components: registry, CI/CD, cluster, pods, secrets, DB Annotations of potential STRIDE threats at each point 📸 Take a screenshot of your threat model diagram to include in workshop materials.\nHands-on threat modeling for a containerized app Choose a containerized application to be deployed in the next lab (e.g., nginx + flask + mongo) Analyze each step: from build → deploy → runtime → networking List threats using STRIDE and provide mitigation actions Step Threat (STRIDE) Mitigation Push image Tampering Sign images, use a private registry Run pod Elevation of Privilege Avoid root, drop unnecessary capabilities Connect DB Information Disclosure Use NetworkPolicy, avoid exposing DB 📘 Expected outcomes:\nUnderstand common container security threats Be able to model threats for your own application Produce a threat model diagram ready to include in the workshop "
},
{
	"uri": "//localhost:1313/2-setup-aws-eks/",
	"title": "Provisioning an EKS Cluster on AWS",
	"tags": [],
	"description": "",
	"content": "Overview In this chapter, you will learn how to provision a real Kubernetes environment on AWS using Amazon EKS. This is a critical prerequisite for deploying container security techniques in the upcoming sections.\n🎯 Learning Objectives By the end of this chapter, you will know how to set up a real Kubernetes environment on AWS using Amazon EKS. This step is essential for practicing container security in later labs.\n📚 What You Will Learn 2.1 Creating an EKS Cluster\nStep-by-step instructions for creating an EKS cluster using eksctl, including IAM roles, VPC setup, and node group configuration.\n2.2 Deploying a Sample Workload\nDeploy a sample application (nginx + backend) on EKS to serve as a testing environment for security labs.\n✅ Outcomes Successfully provision a working Kubernetes cluster on AWS Properly configure IAM permissions and connect to the cluster using kubectl Deploy a sample application to use in upcoming security labs 🚀 Let’s begin with section 2.1 – Creating an EKS Cluster!\n"
},
{
	"uri": "//localhost:1313/4-policy-enforcement-and-runtime-protection/3-automated-remediation/",
	"title": "Automated Response to Violations with Falcosidekick",
	"tags": [],
	"description": "",
	"content": "Main Content Introduction to Falcosidekick Deploying Falcosidekick with Falco Configuring Alert Notifications Testing Alerts Introduction to Falcosidekick Falcosidekick is a service that supports Falco in sending alerts to external systems such as:\nSlack, Discord Email Webhook, Kafka, NATS Prometheus, Grafana, Elasticsearch\u0026hellip; Falcosidekick receives logs from Falco and forwards them according to configuration.\nDeploying Falcosidekick with Falco Step 1: Install Falco with Falcosidekick helm install falcosidekick falcosecurity/falcosidekick --namespace falco --set config.slack.webhookurl=\u0026#34;https://hooks.slack.com/services/xxx\u0026#34; 🔁 You can replace Slack with other webhook URLs as needed.\nStep 2: Check running pods kubectl get pods -n falco ✅ Expected result: 2 pods running: falco and falcosidekick\nConfiguring Alert Notifications You can make additional edits in the values.yaml file if you want to configure multiple alert systems.\nExample: Send to Discord, Prometheus, Webhook\u0026hellip;\nfalcosidekick: config: discord: webhookurl: \u0026#34;https://discord.com/api/webhooks/xxx\u0026#34; webhook: address: \u0026#34;http://your-api-endpoint\u0026#34; Then upgrade the chart:\nhelm upgrade falco falcosecurity/falco -n falco -f values.yaml Testing Alerts Generate anomalous events kubectl run -i --tty attacker --image=alpine -- sh Then run:\ntouch /etc/passwd Check sent alerts Check Falcosidekick logs: kubectl logs -l app=falcosidekick -n falco View notifications on Slack/Discord/Webhook according to configuration. 📸 Take a screenshot of the alert received on Slack or Discord to include in documentation.\nNotes Falcosidekick helps integrate Falco with various alerting systems. Can be extended to send alerts to SIEM, automation systems (XDR, SOAR). "
},
{
	"uri": "//localhost:1313/",
	"title": "Container Security Hardening and Runtime Protection",
	"tags": [],
	"description": "",
	"content": "🔐 Introduction In this section, you will learn how to harden your container environment to minimize security risks and apply runtime monitoring and protection techniques to detect and respond to abnormal behaviors in real time.\nEven when your application is up and running, containers can still be targeted if proper protection layers are not in place. Therefore, combining proactive measures (hardening) with reactive techniques (runtime detection) is critically important.\n🧩 What You Will Learn Best practices for hardening containers and Kubernetes Pods (user, capabilities, readonlyRootFilesystem, seccomp, apparmor, etc.) Using tools like Trivy to scan for vulnerabilities before deployment Installing and configuring Falco to detect suspicious activity in containers Simulating and analyzing real-time attack scenarios Applying runtime security policies to minimize exploitation risks 🎯 Learning Outcomes After completing this section, you will:\nUnderstand how to apply security best practices for containers/Pods Learn the role of monitoring tools like Falco Be able to deploy a runtime protection system in a real-world environment Identify suspicious behaviors early and respond appropriately 🚀 Get ready to harden your container environment against real-world threats!\n"
},
{
	"uri": "//localhost:1313/3-container-security-hardening/",
	"title": "Container Security Hardening",
	"tags": [],
	"description": "",
	"content": "Overview This section focuses on basic container security hardening measures, including:\nScanning container images for vulnerabilities using Trivy Checking compliance with security standards using CIS Benchmark (kube-bench) Applying Kubernetes Network Policies to restrict Pod-to-Pod traffic "
},
{
	"uri": "//localhost:1313/3-container-security-hardening/3-network-policy/",
	"title": "Implementing NetworkPolicy in Kubernetes",
	"tags": [],
	"description": "",
	"content": "Main Content Introduction to NetworkPolicy Creating namespace and sample Pods Creating and applying NetworkPolicy Testing the implementation results Introduction to NetworkPolicy NetworkPolicy is a Kubernetes feature that helps you control network traffic in and out between Pods.\nBy default, all Pods in a cluster can communicate with each other. When you apply NetworkPolicy, you can block all traffic and only allow specific connections.\nKey Concepts Default Behavior: Without NetworkPolicy, all Pod-to-Pod communication is allowed Deny by Default: When a NetworkPolicy is applied, it follows a deny-by-default model Additive Rules: Multiple NetworkPolicies can be applied to the same Pod, with rules being additive Namespace Scoped: NetworkPolicies are applied per namespace Creating namespace and sample Pods Step 1: Create a dedicated namespace kubectl create ns secure-ns Step 2: Create a sample Pod (nginx) kubectl run nginx --image=nginx -n secure-ns --expose --port=80 📌 Note: --expose will automatically create a service to make testing access easier.\nStep 3: Verify the deployment kubectl get pods,svc -n secure-ns You should see both the nginx pod and service created.\nCreating and applying NetworkPolicy Step 4: Create deny-all.yaml file apiVersion: networking.k8s.io/v1 kind: NetworkPolicy metadata: name: deny-all namespace: secure-ns spec: podSelector: {} policyTypes: - Ingress Policy Explanation:\npodSelector: {} - applies to all pods in the namespace policyTypes: [Ingress] - controls incoming traffic No ingress rules specified - denies all ingress traffic Apply the policy:\nkubectl apply -f deny-all.yaml Step 5: Create a test Pod outside the namespace kubectl run busybox --image=busybox:1.28 --rm -it --restart=Never -- sh Try to wget the nginx service:\nwget -qO- nginx.secure-ns.svc.cluster.local ❌ Result: The connection will fail — blocked by NetworkPolicy.\nTesting the implementation results Step 6: Create a policy to allow same-namespace traffic You can add NetworkPolicies that allow specific IPs or namespaces, for example:\napiVersion: networking.k8s.io/v1 kind: NetworkPolicy metadata: name: allow-same-ns namespace: secure-ns spec: podSelector: matchLabels: run: nginx ingress: - from: - podSelector: {} Apply the new policy:\nkubectl apply -f allow-same-ns.yaml Step 7: Test from within the same namespace Create a test pod within the secure-ns namespace:\nkubectl run test-pod --image=busybox:1.28 --rm -it -n secure-ns --restart=Never -- sh Try accessing nginx:\nwget -qO- nginx.secure-ns.svc.cluster.local ✅ If successful → you have successfully controlled traffic as intended.\nAdvanced NetworkPolicy Examples Allow specific namespaces apiVersion: networking.k8s.io/v1 kind: NetworkPolicy metadata: name: allow-from-frontend namespace: secure-ns spec: podSelector: matchLabels: app: backend ingress: - from: - namespaceSelector: matchLabels: name: frontend ports: - protocol: TCP port: 8080 Allow specific external IPs apiVersion: networking.k8s.io/v1 kind: NetworkPolicy metadata: name: allow-external-ips namespace: secure-ns spec: podSelector: {} ingress: - from: - ipBlock: cidr: 10.0.0.0/8 except: - 10.0.1.0/24 Egress policy example apiVersion: networking.k8s.io/v1 kind: NetworkPolicy metadata: name: deny-all-egress namespace: secure-ns spec: podSelector: {} policyTypes: - Egress egress: - to: [] ports: - protocol: UDP port: 53 # Allow DNS Additional Notes Prerequisites and Requirements Ensure your cluster has CNI plugin support for NetworkPolicy (Amazon EKS supports this out of the box) Popular CNI plugins that support NetworkPolicy: Calico - Advanced policy features Cilium - eBPF-based networking Weave Net - Simple setup Amazon VPC CNI - Basic NetworkPolicy support Best Practices Start with Deny-All: Begin with restrictive policies and gradually add exceptions Label Management: Use consistent and meaningful labels for policy selectors Documentation: Document your network policies and their intended behavior Testing: Always test policies in non-production environments first Monitoring: Implement monitoring to detect blocked legitimate traffic Troubleshooting NetworkPolicy # Check if NetworkPolicy is applied kubectl get networkpolicy -n secure-ns # Describe the policy for details kubectl describe networkpolicy deny-all -n secure-ns # Check pod labels kubectl get pods --show-labels -n secure-ns # Test connectivity between pods kubectl exec -n secure-ns test-pod -- nc -zv nginx 80 Integration with Zero Trust Architecture Implementing NetworkPolicy is a crucial step in Zero Trust model for Kubernetes Combine with: Service Mesh (Istio, Linkerd) for application-layer security Pod Security Standards for runtime security RBAC for API access control Admission Controllers for policy enforcement Monitoring and Observability Consider implementing tools for NetworkPolicy monitoring:\nCilium Hubble - Network observability Calico Enterprise - Policy visualization Kubernetes Network Policy Viewer - Policy visualization tools 🎯 Conclusion: NetworkPolicy is essential for implementing microsegmentation in Kubernetes. Start with restrictive policies, test thoroughly, and gradually refine your network security posture to achieve Zero Trust networking.\n"
},
{
	"uri": "//localhost:1313/4-policy-enforcement-and-runtime-protection/",
	"title": "Policy Enforcement &amp; Runtime Protection",
	"tags": [],
	"description": "",
	"content": "Overview This section focuses on advanced container security techniques, specifically:\nEnforcing runtime behavior using Falco to detect suspicious activity Implementing Policy-as-Code with Open Policy Agent (OPA) to control deployment and access Combining runtime detection and policy enforcement for proactive defense "
},
{
	"uri": "//localhost:1313/5-security-monitoring-and-response/",
	"title": "Policy Management with OPA Gatekeeper (Windows)",
	"tags": [],
	"description": "",
	"content": "Main Content Introduction to OPA Gatekeeper Installing Helm on Windows Deploying Gatekeeper on EKS Writing and Testing Policies Introduction OPA (Open Policy Agent) is a tool used to enforce policies across various systems. Gatekeeper is an extension project that integrates OPA into Kubernetes.\nGatekeeper enables:\nDeployment control through constraint rules Logging of policy violations Blocking unsafe configurations (e.g., disallowing images with the :latest tag) Installing Helm on Windows Visit https://github.com/helm/helm/releases Download: helm-v3.x.x-windows-amd64.zip Unzip the file → Extract helm.exe Create a folder, e.g., C:\\Program Files\\helm\\, and move helm.exe there Add the folder path to Environment Variables \u0026gt; Path Open PowerShell/CMD and check: helm version "
},
{
	"uri": "//localhost:1313/5-security-monitoring-and-response/2-security-dashboard/",
	"title": "",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "//localhost:1313/5-security-monitoring-and-response/3-penetration-testing/",
	"title": "",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "//localhost:1313/categories/",
	"title": "Categories",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "//localhost:1313/tags/",
	"title": "Tags",
	"tags": [],
	"description": "",
	"content": ""
}]